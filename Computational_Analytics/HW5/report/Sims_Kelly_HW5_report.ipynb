{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 {-}\n",
    "## SVM {-}\n",
    "\n",
    "#### (a) Explain why can we set the margin c=1 to derive the SVM formulation {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "Given that the optimization problem for SVM is \n",
    "\n",
    "$$\n",
    "max_{w,b} \\frac{2c}{||w||}\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "y^i(w^Tx^i + b) \\geq c, \\forall i\n",
    "$$\n",
    "\n",
    "We are able to apply a constant to the constraint without changing the overall problem. E.g. dividing **w** and **b** by a factor of **a**\n",
    "\n",
    "$$\n",
    "y^i(\\frac{w^Tx^i}{a} + \\frac{b}{a}) \\geq c \\ \\ \\ \\ \\ \\ \\text{(1)} \\\\\n",
    "= y^i(w^Tx^i + b) \\geq (c * a) \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "Where **a** is some arbitrary constant. Since dividing **w** and **b** by constant **a** (1) is equivalent to multiplying **c** by constant **a**, we can choose **a** to be \n",
    "\n",
    "$$\n",
    "\\frac{1}{c}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "c * \\frac{1}{c} = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Using Lagrangian dual formulation, show that the weight vector can be represented as {-}\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{n} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "#### where $\\alpha_i \\geq 0$ are the dual variables. What does this imply in terms of how to relate data to w? {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "Given the alternate form of the optimization problem for SVM\n",
    "\n",
    "$$\n",
    "min_{w,b} \\frac{1}{2}w^Tw\n",
    "$$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$$\n",
    "1 - y^i(w^Tx^i + b) \\leq 0, \\forall i\n",
    "$$\n",
    "\n",
    "The lagragian seeks to find where the gradient of the objective is not equal, but proportional to the gradient of the constraint(s). That is, where the constraint is tangent with the objective. \n",
    "\n",
    "$$\n",
    "\\text{objective} = \\alpha * \\text{constraint}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the gradient proportionality constant, aka the lagrangian multiplier. From this step, we can convert the objective and the constraint to the lagrangian form of\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(w,b,\\alpha) = \\frac{1}{2} w^Tw + \\sum_{i=1}^m \\alpha_i(1-y_i(w^Tx_i+b))\n",
    "$$\n",
    "\n",
    "We have now converted our constrained optimization problem into an unconstrained quadratic form, which is solved by taking the gradient of $\\mathscr{L}$ and setting equal to 0\n",
    "\n",
    "$$\n",
    "\\triangledown \\mathscr{L} = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial b} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial \\alpha}\\end{bmatrix} = \\boldsymbol{0}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial w} = 0 \\\\ = \\frac{\\partial}{\\partial w} \\frac{1}{2}w^Tw + \\frac{\\partial}{\\partial w} \\sum_{i=1}^m \\alpha(1-y_i(w^Tx_i+b)) = 0 \\\\\n",
    " = w - \\sum_{i=1}^m \\alpha_i y_i x_i = 0 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{w = \\sum_{i=1}^m \\alpha_i y_i x_i }\n",
    "$$\n",
    "\n",
    "From this we can see the vector **w** is a linear combination of the feature vectors and the lagrangian proportionality constant. This means that w is derived entirely from the feature vectors themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Explain why only the data points on the \"margin\" will contribute to the sum above, i.e., playing a role in defining w. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "The KKT condition which specifies the criteria for the saddle point of $\\mathscr{L}$ states that \n",
    "\n",
    "$$\n",
    "\\alpha_i g_i(w) = 0 \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$\n",
    "\n",
    "where (1) is the constraint from above\n",
    "\n",
    "$$\n",
    "\\alpha_i(1-y_i(w^Tx_i+b)) = 0 \\ \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "in order for (2) to be true, notice that for a given point, if 1 minus its projection onto **w** is non-zero, meaning the point is not on the margin,\n",
    "\n",
    "$$\n",
    "1-y_i(w^Tx_i+b) < 0\n",
    "$$\n",
    "\n",
    "then $\\alpha$ must be equal to 0 in those instances. Conversely, if 1 minus the projection of a point onto **w** is zero, \n",
    "\n",
    "$$\n",
    "1-y_i(w^Tx_i+b) = 0\n",
    "$$\n",
    "\n",
    "then $\\alpha_i > 0$. Therefore, only those points who have a non-zero alpha lay on the margin. When plugging all these values back into the summation above in the previous section, we can see those points which do not lay on the margin will have a multplicative factor of zero for $\\alpha_i$ and those which do lay on the margin will have a non-zero multplicative factor. Thus, only those points that lay on the margin contribute to the summation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Suppose we only have four training examples in two dimensions as shown in Fig. The positive samples at x 1 = (0, 0), x 2 = (2, 2) and negative samples at x 3 = (h, 1) and x 4 = (0, 3). {-}\n",
    "\n",
    "![svm](svm.png)\n",
    "\n",
    "#### (i.) For what range of parameter h > 0, the training points are still linearly separable? {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "If we draw a straight line between the two blue cross points at (0,0) & (2,2), and solve for the slope (m) in $y=mx + b$\n",
    "\n",
    "$$\n",
    "y = \\frac{(2-0)}{(2-0)} x + 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = x \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$\n",
    "\n",
    "we can see that the points are linearly separable up until the point in which y = x (1) to the left side of the line. Since the point **(h,1)** has a y of 1, it will be that the points remain separable for h < 1. Conversely, if we draw a line from the point in the upper left hand corner at **(0,3)** to point **(2,2)** and solve for the equation $y=mx + b$\n",
    "\n",
    "$$\n",
    "y = \\frac{(2-3)}{(2-0)}x + 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\frac{-1}{2}x + 3 \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "We plug in 1 for y into (2) and solve for x\n",
    "\n",
    "$$\n",
    "1 = \\frac{-1}{2}x + 3 \\\\\n",
    "-2(1 - 3) = x \\\\\n",
    "x = 4\n",
    "$$\n",
    "\n",
    "Hence, when h > 4, the graph is linearly seperable to the right side. Therefore, the points are still linearly seperable under the conditions of\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{cases} \n",
    "0 \\leq h < 1 \\\\\n",
    "h > 4\n",
    "\\end{cases}\n",
    "}\n",
    "$$\n",
    "\n",
    "#### (ii.) Does the orientation of the maximum margin decision boundary chage as h changes, when the points are seperable? {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "for the first interval $0 \\leq h < 1$, we can see from equation (1) that the slope is positive 1. Therefore the margin runs in an upward trend from the bottom left to the upper right. For the second interval, we can see from equation (2) that the slope is negative $\\frac{-1}{2}$, meaning the margin would run in a downward trend from the upper left corner to the lower right corner of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 {-}\n",
    "## Multi-class classification for MNIST data set, comparison {-}\n",
    "\n",
    "This question is to compare different classifiers and their performance for multi-class classifications\n",
    "on the complete MNIST dataset at http://yann.lecun.com/exdb/mnist/. You can find the data\n",
    "1file mnist 10digits.mat in the homework folder. The MNIST database of handwritten digits has a\n",
    "training set of 60,000 examples, and a test set of 10,000 examples. We will compare KNN, logistic\n",
    "regression, SVM, kernel SVM, and neural networks. We suggest to use Scikit-learn, which is\n",
    "a commonly-used and powerful Python library with various machine learning tools. But you can also\n",
    "use other similar libraries in other programming languages of your choice to perform the tasks. Below\n",
    "are some tips.\n",
    "* We suggest you to “standardize” the features before training the classifiers, by dividing the values of the features by 255 (thus map the range of the features from [0, 255] to [0, 1]).\n",
    "* You may adjust the number of neighbors K used in KNN to have a reasonable result (you may use cross validation but it is not required; any reasonable tuning to get good result is acceptable).\n",
    "* You may use a neural networks function sklearn.neural network with hidden layer sizes = (20, 10).\n",
    "* For kernel SVM, you may use radial basis function kernel, and a heuristic called \"median trick\": choose the parameter of the kernel $K(x,x') = exp{-||x - x'||^2 / (2\\sigma^2)}$. Choose the bandwidth as $\\sigma = \\sqrt{M/2}$ where M = the median of ${||x^i - x^j||^2, 1 \\leq i, j\\leq m', i \\neq j}$ for pairs of training samples. Here you can randomly choose m' = 1000 samples from training data to use for the \"median trick\"\n",
    "* For KNN and SVM, you can randomly downsample the training data to size m = 5000, to improve\n",
    "computation efficiency.\n",
    "Train the classifiers on training dataset and evaluate on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report confusion matrix, precision, recall, and F-1 score for each of the classifiers. For precision, recall, and F-1 score of each classifier, we will need to report these for each of the digits. So you can create a table for this. For this question, each of the 5 classifier, KNN, logistic regression, SVM, kernel SVM, and neural networks {-}\n",
    "\n",
    "##### {-} **Answer**\n",
    "\n",
    "**KNN**\n",
    "--------\n",
    "\n",
    "![KNN_CM](cm_KNN.png)\n",
    "![KNN_prf](prf_KNN.png)\n",
    "\n",
    "\n",
    "**Logistic Regression**\n",
    "--------\n",
    "\n",
    "![lr_cm](cm_Logistic_Regression.png)\n",
    "![lr_prf](prf_Logistic_Regression.png)\n",
    "\n",
    "\n",
    "**Neural Network**\n",
    "--------\n",
    "\n",
    "![nn_cm](cm_Neural_Network.png)\n",
    "![nn_prf](prf_Neural_Network.png)\n",
    "\n",
    "\n",
    "**SVM**\n",
    "-------\n",
    "\n",
    "![svm_cm](cm_SVM.png)\n",
    "![svm_prf](prf_SVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on the performance of the classifier and give your explanation why some of them perform better than the others. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "All models perform exceptionally well given the high dimensionality of the input feature space. However, KNN and NN performs better than Logistic Regression and SVM. This is probably due in part that the latter models inherently are linear classifiers. Although the RBF kernel is being applied to SVM to conform to the nonlinearity of the classes, the data isn't very easily separated. KNN doesn't attempt to find a parametric function that maximizes the separation between the classes, this is one reason why it performs better. It attempts to find the classes based on how similar they are. In this respect, similarity is deteremined from distance in euclidean space. Conversely, with respect to the NN, it has the capability to learn extremely complex non linear decision functions. Due to the high dimensionality and lack of clear separability between the classes, the NN has the ability to learn an efficient non linear function for classificaiton. Hence why it performs better than SVM and Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
