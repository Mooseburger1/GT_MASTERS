{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 {-}\n",
    "## SVM {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Explain why can we set the margin c=1 to derive the SVM formulation {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "Given that the optimization problem for SVM is \n",
    "\n",
    "$$\n",
    "max_{w,b} \\frac{2c}{||w||}\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "y^i(w^Tx^i + b) \\geq c, \\forall i\n",
    "$$\n",
    "\n",
    "We are able to apply a constant to the constraint without changing the overall problem. E.g. dividing **w** and **b** by a factor of **a**\n",
    "\n",
    "$$\n",
    "y^i(\\frac{w^Tx^i}{a} + \\frac{b}{a}) \\geq c \\ \\ \\ \\ \\ \\ \\text{(1)} \\\\\n",
    "= y^i(w^Tx^i + b) \\geq (c * a) \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "Where **a** is some arbitrary constant. Since dividing **w** and **b** by constant **a** (1) is equivalent to multiplying **c** by constant **a**, we can choose **a** to be \n",
    "\n",
    "$$\n",
    "\\frac{1}{c}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "c * \\frac{1}{c} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Using Lagrangian dual formulation, show that the weight vector can be represented as\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{n} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "#### where $\\alpha_i \\geq 0$ are the dual variables. What does this imply in terms of how to relate data to w?\n",
    "\n",
    "##### **Answer**\n",
    "\n",
    "Given the alternate form of the optimization problem for SVM\n",
    "\n",
    "$$\n",
    "min_{w,b} \\frac{1}{2}w^Tw\n",
    "$$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$$\n",
    "1 - y^i(w^Tx^i + b) \\leq 0, \\forall i\n",
    "$$\n",
    "\n",
    "The lagragian seeks to find where the gradient of the objective is not equal, but proportional to the gradient of the constraint(s). That is, where the constraint is tangent with the objective. \n",
    "\n",
    "$$\n",
    "\\text{objective} = \\alpha * \\text{constraint}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the gradient proportionality constant, aka the lagrangian multiplier. From this step, we can convert the objective and the constraint to the lagrangian form of\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(w,b,\\alpha) = \\frac{1}{2} w^Tw + \\sum_{i=1}^m \\alpha_i(1-y_i(w^Tx_i+b))\n",
    "$$\n",
    "\n",
    "We have now converted our constrained optimization problem into an unconstrained quadratic form, which is solved by taking the gradient of $\\mathscr{L}$ and setting equal to 0\n",
    "\n",
    "$$\n",
    "\\triangledown \\mathscr{L} = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial b} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial \\alpha}\\end{bmatrix} = \\boldsymbol{0}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial w} = 0 \\\\ = \\frac{\\partial}{\\partial w} \\frac{1}{2}w^Tw + \\frac{\\partial}{\\partial w} \\sum_{i=1}^m \\alpha(1-y_i(w^Tx_i+b)) = 0 \\\\\n",
    " = w - \\sum_{i=1}^m \\alpha_i y_i x_i = 0 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{w = \\sum_{i=1}^m \\alpha_i y_i x_i }\n",
    "$$\n",
    "\n",
    "From this we can see the vector **w** is a linear combination of the feature vectors and the lagrangian proportionality constant. This means that w is derived entirely from the feature vectors themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Explain why only the data points on the \"margin\" will contribute to the sum above, i.e., playing a role in defining w.\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "The KKT condition which specifies the criteria for the saddle point of $\\mathscr{L}$ states that \n",
    "\n",
    "$$\n",
    "\\alpha_i g_i(w) = 0 \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$\n",
    "\n",
    "where (1) is the constraint from above\n",
    "\n",
    "$$\n",
    "\\alpha_i(1-y_i(w^Tx_i+b)) = 0 \\ \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "in order for (2) to be true, notice that for a given point, if 1 minus its projection onto **w** is non-zero, meaning the point is not on the margin,\n",
    "\n",
    "$$\n",
    "1-y_i(w^Tx_i+b) < 0\n",
    "$$\n",
    "\n",
    "then $\\alpha$ must be equal to 0 in those instances. Conversely, if 1 minus the projection of a point onto **w** is zero, \n",
    "\n",
    "$$\n",
    "1-y_i(w^Tx_i+b) = 0\n",
    "$$\n",
    "\n",
    "then $\\alpha_i > 0$. Therefore, only those points who have a non-zero alpha lay on the margin. When plugging all these values back into the summation above, we can see those points which do not lay on the margin will have a multplicative factor of zero for $\\alpha_i$ and those which do lay on the margin will have a non-zero multplicative factor. Thus, only those points that lay on the margin contribute to the summation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
