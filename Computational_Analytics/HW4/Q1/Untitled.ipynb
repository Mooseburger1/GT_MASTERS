{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "## Basic optimization\n",
    "\n",
    "Consider a simplified logistic regression problem. Given m training samples (x i , y i ), i = 1, . . . , m.\n",
    "The data x i ∈ R (note that we only have one feature for each sample), and y i ∈ {0, 1}. To fit a\n",
    "logistic regression model for classification, we solve the following optimization problem, where θ ∈ R\n",
    "is a parameter we aim to find:\n",
    "\n",
    "$$\n",
    "max_{\\theta}\\ell(\\theta)\n",
    "$$\n",
    "\n",
    "where the log-likelihood function\n",
    "\n",
    "$$\n",
    "\\ell (\\theta) = \\sum_{i=1}^m \\{ - log(1 + exp\\{-\\theta x^i\\}) + (y^i - 1)\\theta x^i\\}\\ \\ \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show step-by-step mathematical derivation for the gradient of the cost function $\\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let **X** be a vector in $\\mathbb{R}^m$ that represents $x^i$ for i = 1, ..., m. Substituting **X** in (1) and removing the summation yields:\n",
    "\n",
    "$$\n",
    "-log( 1 + exp\\{-\\theta X\\}) + (y^i - 1) \\theta X \\ \\ \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "Taking the derivative of (2) w.r.t $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial \\ell}{\\partial \\theta} \\theta \\cdot X(y-1) - \\frac{\\partial \\ell}{\\partial \\theta} log(exp\\{-\\theta X\\} + 1 )\n",
    "$$\n",
    "$$\n",
    "= X(y-1) - \\frac{\\frac{\\partial \\ell}{\\partial \\theta} exp\\{-\\theta X\\} + \\frac{\\partial \\ell}{\\partial \\theta} 1}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "$$\n",
    "= X(y-1) - \\frac{(-X \\cdot \\frac{\\partial \\ell}{\\partial \\theta} \\theta) \\cdot exp\\{-\\theta X\\}}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "Which yields\n",
    "$$\n",
    "\\boxed{X(y-1) + \\frac{exp\\{-\\theta X\\}X}{exp\\{-\\theta X\\} + 1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Write a pseudo-code for performing gradient descent to find the optimizer $θ^*$. This is essentially what the training procedure does.\n",
    "\n",
    "\n",
    "* Given m examples of feature vector **X** and label vector **Y**, learning rate $\\alpha$\n",
    "* Randomly initialize **$\\theta$** , bias **b**\n",
    "* Repeat until convergence:\n",
    "    * Predict $\\hat{y}$ = h(X) = z($\\theta X + b$)\n",
    "    * Calculate cost $\\ell(\\hat{y}, y)$\n",
    "    * Calculate $\\nabla \\ell(\\theta)$ = = $X(Y - 1) + \\frac{exp(-\\theta X)X}{exp(-\\theta X)+1}$\n",
    "    * Update $\\theta \\leftarrow \\theta - \\alpha \\nabla\\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Write the pseudo-code for performing the stochastic gradient descent algorithm to solve the training of logistic regression problem (1). Please explain the difference between gradient descent and stochastic gradient descent for training logistic regression.\n",
    "\n",
    "* Given m examples of $(x^{(i)},y^{(i)})$, learning rate $\\alpha$\n",
    "* Randomly initialize **$\\theta$** , bias **b**\n",
    "* Repeat until convergence:\n",
    "    * For i = 1, 2, ..., m\n",
    "        * Predict $\\hat{y}^{(i)}$ = $h(x^{(i)})$ = z($\\theta x^{(i)} + b$)\n",
    "        * Calculate cost $\\ell(\\hat{y}^{(i)}, y^{(i)})$\n",
    "        * Calculate $\\nabla \\ell(\\theta)$ = $x^{(i)}(y^{(i)} - 1) + \\frac{exp(-\\theta x^{(i)})x^{(i)}}{exp(-\\theta x^{(i)})+1}$\n",
    "        * Update $\\theta \\leftarrow \\theta - \\alpha \\nabla\\ell(\\theta)$\n",
    "        \n",
    "        \n",
    "Gradient descent computes the gradients after iterating through all m training observations. With large training sets, this can become computationally expensive to do in the event of a vectorized implementation. Run time for an iterative approach can take a long time to loop through all observations as well. Stochastic gradient descent, on the other hand, either samples a small subset of traiing observations to compute the gradient. Or in the case of \"online\" learning, the gradient is calculated and the weights are updated after each observation seen (as shown above in the pseudo code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) We will show that the training problem in basic logistic regression problemis concave. Derive the Hessian matrix of (θ) and based on this, show the training problem (1) is concave (note that in this case, since we only have one feature, the Hessian matrixis just ascalar). Explain why the problem can be solved efficiently and gradient descent will achieve a unique global optimizer, as we discussed in class.\n",
    "\n",
    "Taking the derivative of the result from part **(a)** yields:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial'' \\ell}{\\partial \\theta''} = \\frac{\\partial' \\ell}{\\partial \\theta'} X(y-1) + \\frac{exp\\{-\\theta X\\}X}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{= - \\frac{exp(-\\theta X)X^2}{(1 + exp(-\\theta X))^2}\\ \\ \\ \\ \\ \\ \\ \\text{(1)}}\n",
    "$$\n",
    "\n",
    "Since X $\\in \\mathbb{R}^{mx1}$ then there is a single theta in the Logistic Regression problem. Because of this, the hessian turns out to be a single value. In this instance, that value will be negative as seen above. Any real value entered in (1) above will be negative. The hessian of a function of a single variable is concave if its second derivative is negative everywhere. A concave optimization problem and conversely a convex optimization problem when minimzing an optjective function, will have a unique global solution and can be solved efficiently without fear of being stuck in a local minimum (or maximum). This is in contrast to a non-convex (non-concave) optimization problem, such as LP or integer optimization. These types of problems can get stuck in a local minimum and/or maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "## Comparing Bayes, logistic, and KNN classifiers\n",
    "In lectures, we learn three different classifiers. This question is to implement and compare them.\n",
    "Python users, please feel free to use Scikit-learn, which is a commonly-used and powerful Python library\n",
    "with various machine learning tools. But you can also use other similar libraries in other languages of\n",
    "your choice to perform the tasks.\n",
    "Part One (Divorce classification/prediction). (30 points)\n",
    "This dataset is about participants who completed the personal information form and a divorce predic-\n",
    "tors scale.\n",
    "The data is a modified version of the publicly available at\n",
    "https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set (by injecting noise so\n",
    "you will not get the exactly same results as on UCI website). The dataset marriage.csv is contained\n",
    "in the homework folder. There are 170 participants and 54 attributes (or predictor variables) that are\n",
    "all real-valued. The last column of the CSV file is label y (1 means “divorce”, 0 means “no divorce”).\n",
    "Each column is for one feature (predictor variable), and each row is a sample (participant). A detailed\n",
    "1explanation for each feature (predictor variable) can be found at the website link above. Our goal is\n",
    "to build a classifier using training data, such that given a test sample, we can classify (or essentially\n",
    "predict) whether its label is 0 (“no divorce”) or 1 (“divorce”).\n",
    "Build three classifiers using (Naive Bayes, Logistic Regression, KNN). Use the first 80% data for\n",
    "training and the remaining 20% for testing. If you use scikit-learn you can use train test split to split\n",
    "the dataset.\n",
    "Remark: Please note that, here, for Naive Bayes, this means that we have to estimate the variance for\n",
    "each individual feature from training data. When estimating the variance, if the variance is zero to\n",
    "close to zero (meaning that there is very little variability in the feature), you can set the variance to\n",
    "be a small number, e.g., $\\epsilon = 10^{−3}$. We do not want to have include zero or nearly variance in Naive\n",
    "Bayes. This tip holds for both Part One and Part Two of this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
