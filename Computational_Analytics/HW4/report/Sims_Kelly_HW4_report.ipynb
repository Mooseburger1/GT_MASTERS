{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 {-}\n",
    "## Basic optimization {-}\n",
    "\n",
    "Consider a simplified logistic regression problem. Given m training samples (x i , y i ), i = 1, . . . , m.\n",
    "The data x i ∈ R (note that we only have one feature for each sample), and y i ∈ {0, 1}. To fit a\n",
    "logistic regression model for classification, we solve the following optimization problem, where θ ∈ R\n",
    "is a parameter we aim to find:\n",
    "\n",
    "$$\n",
    "max_{\\theta}\\ell(\\theta)\n",
    "$$\n",
    "\n",
    "where the log-likelihood function\n",
    "\n",
    "$$\n",
    "\\ell (\\theta) = \\sum_{i=1}^m \\{ - log(1 + exp\\{-\\theta x^i\\}) + (y^i - 1)\\theta x^i\\}\\ \\ \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show step-by-step mathematical derivation for the gradient of the cost function $\\ell(\\theta)$ {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let **X** be a vector in $\\mathbb{R}^m$ that represents $x^i$ for i = 1, ..., m. Substituting **X** in (1) and removing the summation yields:\n",
    "\n",
    "$$\n",
    "-log( 1 + exp\\{-\\theta X\\}) + (y^i - 1) \\theta X \\ \\ \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "Taking the derivative of (2) w.r.t $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial \\ell}{\\partial \\theta} \\theta \\cdot X(y-1) - \\frac{\\partial \\ell}{\\partial \\theta} log(exp\\{-\\theta X\\} + 1 )\n",
    "$$\n",
    "$$\n",
    "= X(y-1) - \\frac{\\frac{\\partial \\ell}{\\partial \\theta} exp\\{-\\theta X\\} + \\frac{\\partial \\ell}{\\partial \\theta} 1}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "$$\n",
    "= X(y-1) - \\frac{(-X \\cdot \\frac{\\partial \\ell}{\\partial \\theta} \\theta) \\cdot exp\\{-\\theta X\\}}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "Which yields\n",
    "$$\n",
    "\\boxed{X(y-1) + \\frac{exp\\{-\\theta X\\}X}{exp\\{-\\theta X\\} + 1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Write a pseudo-code for performing gradient descent to find the optimizer $θ^*$. This is essentially what the training procedure does. {-}\n",
    "\n",
    "\n",
    "* Given m examples of feature vector **X** and label vector **Y**, learning rate $\\alpha$\n",
    "* Randomly initialize **$\\theta$** , bias **b**\n",
    "* Repeat until convergence:\n",
    "    * Predict $\\hat{y}$ = h(X) = z($\\theta X + b$)\n",
    "    * Calculate cost $\\ell(\\hat{y}, y)$\n",
    "    * Calculate $\\nabla \\ell(\\theta)$ = = $X(Y - 1) + \\frac{exp(-\\theta X)X}{exp(-\\theta X)+1}$\n",
    "    * Update $\\theta \\leftarrow \\theta - \\alpha \\nabla\\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Write the pseudo-code for performing the stochastic gradient descent algorithm to solve the training of logistic regression problem (1). Please explain the difference between gradient descent and stochastic gradient descent for training logistic regression. {-}\n",
    "\n",
    "* Given m examples of $(x^{(i)},y^{(i)})$, learning rate $\\alpha$\n",
    "* Randomly initialize **$\\theta$** , bias **b**\n",
    "* Repeat until convergence:\n",
    "    * For i = 1, 2, ..., m\n",
    "        * Predict $\\hat{y}^{(i)}$ = $h(x^{(i)})$ = z($\\theta x^{(i)} + b$)\n",
    "        * Calculate cost $\\ell(\\hat{y}^{(i)}, y^{(i)})$\n",
    "        * Calculate $\\nabla \\ell(\\theta)$ = $x^{(i)}(y^{(i)} - 1) + \\frac{exp(-\\theta x^{(i)})x^{(i)}}{exp(-\\theta x^{(i)})+1}$\n",
    "        * Update $\\theta \\leftarrow \\theta - \\alpha \\nabla\\ell(\\theta)$\n",
    "        \n",
    "        \n",
    "Gradient descent computes the gradients after iterating through all m training observations. With large training sets, this can become computationally expensive to do in the event of a vectorized implementation. Run time for an iterative approach can take a long time to loop through all observations as well. Stochastic gradient descent, on the other hand, either samples a small subset of traiing observations to compute the gradient. Or in the case of \"online\" learning, the gradient is calculated and the weights are updated after each observation seen (as shown above in the pseudo code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) We will show that the training problem in basic logistic regression problemis concave. Derive the Hessian matrix of (θ) and based on this, show the training problem (1) is concave (note that in this case, since we only have one feature, the Hessian matrixis just ascalar). Explain why the problem can be solved efficiently and gradient descent will achieve a unique global optimizer, as we discussed in class. {-}\n",
    "\n",
    "Taking the derivative of the result from part **(a)** yields:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial'' \\ell}{\\partial \\theta''} = \\frac{\\partial' \\ell}{\\partial \\theta'} X(y-1) + \\frac{exp\\{-\\theta X\\}X}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{= - \\frac{exp(-\\theta X)X^2}{(1 + exp(-\\theta X))^2}\\ \\ \\ \\ \\ \\ \\ \\text{(1)}}\n",
    "$$\n",
    "\n",
    "Since X $\\in \\mathbb{R}^{mx1}$ then there is a single theta in the Logistic Regression problem. Because of this, the hessian turns out to be a single value. In this instance, that value will be negative as seen above. Any real value entered in (1) above will be negative. The hessian of a function of a single variable is concave if its second derivative is negative everywhere. A concave optimization problem and conversely a convex optimization problem when minimzing an optjective function, will have a unique global solution and can be solved efficiently without fear of being stuck in a local minimum (or maximum). This is in contrast to a non-convex (non-concave) optimization problem, such as LP or integer optimization. These types of problems can get stuck in a local minimum and/or maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 {-}\n",
    "## Comparing Bayes, logistic, and KNN classifiers {-}\n",
    "In lectures, we learn three different classifiers. This question is to implement and compare them.\n",
    "Python users, please feel free to use Scikit-learn, which is a commonly-used and powerful Python library\n",
    "with various machine learning tools. But you can also use other similar libraries in other languages of\n",
    "your choice to perform the tasks.\n",
    "\n",
    "\n",
    "**Part One (Divorce classification/prediction).**\n",
    "This dataset is about participants who completed the personal information form and a divorce predic-\n",
    "tors scale.\n",
    "The data is a modified version of the publicly available at\n",
    "https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set (by injecting noise so\n",
    "you will not get the exactly same results as on UCI website). The dataset marriage.csv is contained\n",
    "in the homework folder. There are 170 participants and 54 attributes (or predictor variables) that are\n",
    "all real-valued. The last column of the CSV file is label y (1 means “divorce”, 0 means “no divorce”).\n",
    "Each column is for one feature (predictor variable), and each row is a sample (participant). A detailed\n",
    "1explanation for each feature (predictor variable) can be found at the website link above. Our goal is\n",
    "to build a classifier using training data, such that given a test sample, we can classify (or essentially\n",
    "predict) whether its label is 0 (“no divorce”) or 1 (“divorce”).\n",
    "Build three classifiers using (Naive Bayes, Logistic Regression, KNN). Use the first 80% data for\n",
    "training and the remaining 20% for testing. If you use scikit-learn you can use train test split to split\n",
    "the dataset.\n",
    "Remark: Please note that, here, for Naive Bayes, this means that we have to estimate the variance for\n",
    "each individual feature from training data. When estimating the variance, if the variance is zero to\n",
    "close to zero (meaning that there is very little variability in the feature), you can set the variance to\n",
    "be a small number, e.g., $\\epsilon = 10^{−3}$. We do not want to have include zero or nearly variance in Naive\n",
    "Bayes. This tip holds for both Part One and Part Two of this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Report testing accuracy for each of the three classifiers. Comment on their perfor- mance: which performs the best and make a guess why they perform the best in this setting. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![acc](p1_acc.png)\n",
    "\n",
    "![cm](p1_conf.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "All 3 classifier perform extremely well on this dataset. They each got the same accuracy on the test set of 0.971. They each failed to classify only one observation incorrectly. As we will see in the next part below, this is due to how well the data itself is linearly seperable. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Now perform PCA to project the data into two-dimensional space. Plot the data points and decision boundary of each classifier. Comment on the difference between the decision boundary for the three classifiers. Please clearly represent the data points with different labels using different colors. {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![lr](lr.png)\n",
    "![knn](knn.png)\n",
    "![nb](nb.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with the data projected down to the first 2 principal components, the strong decision boundary can be observed. With respect to the training set, there are 3 observations that are extremely close to the other class. This brings the decision boundary over to the left drastically. In doing so however, we can see it allows the models to correctly classify the lone isolated observation in the test sets for logistic regression and KNN Naive Bayes would be the sole model to fail to properly classify the single point at the boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part Two (Handwritten digits classification).** Repeat the above using the MNIST\n",
    "Data in our previous homework. Here, give “digit” 6 label y = 1, and give “digit” 2 label y = 0. All\n",
    "the pixels in each image will be the feature (predictor variables) for that sample (i.e., image). Our goal\n",
    "is to build classifier to such that given a new test sample, we can tell is it a 2 or a 6. Using the first\n",
    "80% of the samples for training and remaining 20% for testing.\n",
    "\n",
    "\n",
    "#### (a) Report testing accuracy for each of the three classifiers. Comment on their perfor- mance: which performs the best and make a guess why they perform the best in this setting. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![p2_acc](p2_acc.png)\n",
    "\n",
    "![p2_cm](p2_cm.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Again, all 3 models perform well in classifying each observation. However KNN performs the best of the group. As we will see in the next section, this is due to KNN's ability to create a decision boundary that is non linear. Because there is observation mixing in the high dimensional space, it is not linearly separable. This is why LR doesn't perform as well. Also because of this mixing, this effects the conjugate priors utilized in the Naive Bayes method. KNN creates a decision boundary, and potentially decision islands, and this is why it performs better in this scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Now perform PCA to project the data into two-dimensional space. Plot the data points and decision boundary of each classifier. Comment on the difference between the decision boundary for the three classifiers. Please clearly represent the data points with different labels using different colors. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![lr2](lr2.png)\n",
    "\n",
    "![knn2](knn2.png)\n",
    "\n",
    "![nb2](nb2.png)\n",
    "\n",
    "\n",
    "After reducing the data to the first two principal componenets and plotting the decision boundary, we can see these \"decision islands\" as discussed in the prior section. Both Naive Bayes and Logistic regression attempt to create a decision line, whereas KNN creates a highly stochastic decision boundary with respect to where the data points lie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
