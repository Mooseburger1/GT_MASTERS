{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "## Basic optimization\n",
    "\n",
    "Consider a simplified logistic regression problem. Given m training samples (x i , y i ), i = 1, . . . , m.\n",
    "The data x i ∈ R (note that we only have one feature for each sample), and y i ∈ {0, 1}. To fit a\n",
    "logistic regression model for classification, we solve the following optimization problem, where θ ∈ R\n",
    "is a parameter we aim to find:\n",
    "\n",
    "$$\n",
    "max_{\\theta}\\ell(\\theta)\n",
    "$$\n",
    "\n",
    "where the log-likelihood function\n",
    "\n",
    "$$\n",
    "\\ell (\\theta) = \\sum_{i=1}^m \\{ - log(1 + exp\\{-\\theta x^i\\}) + (y^i - 1)\\theta x^i\\}\\ \\ \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show step-by-step mathematical derivation for the gradient of the cost function $\\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let **X** be a vector in $\\mathbb{R}^m$ that represents $x^i$ for i = 1, ..., m. Substituting **X** in (1) and removing the summation yields:\n",
    "\n",
    "$$\n",
    "-log( 1 + exp\\{-\\theta X\\}) + (y^i - 1) \\theta X \\ \\ \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "Taking the derivative of (2) w.r.t $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial \\ell}{\\partial \\theta} \\theta \\cdot X(y-1) - \\frac{\\partial \\ell}{\\partial \\theta} log(exp\\{-\\theta X\\} + 1 )\n",
    "$$\n",
    "$$\n",
    "= X(y-1) - \\frac{\\frac{\\partial \\ell}{\\partial \\theta} exp\\{-\\theta X\\} + \\frac{\\partial \\ell}{\\partial \\theta} 1}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "$$\n",
    "= X(y-1) - \\frac{(-X \\cdot \\frac{\\partial \\ell}{\\partial \\theta} \\theta) \\cdot exp\\{-\\theta X\\}}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "Which yields\n",
    "$$\n",
    "\\boxed{X(y-1) + \\frac{exp\\{-\\theta X\\}X}{exp\\{-\\theta X\\} + 1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Write a pseudo-code for performing gradient descent to find the optimizer $θ^*$. This is essentially what the training procedure does.\n",
    "\n",
    "\n",
    "* Given m examples of feature vector **X** and label vector **Y**, learning rate $\\alpha$\n",
    "* Randomly initialize **$\\theta$** , bias **b**\n",
    "* Repeat until convergence:\n",
    "    * Predict $\\hat{y}$ = h(X) = z($\\theta X + b$)\n",
    "    * Calculate cost $\\ell(\\hat{y}, y)$\n",
    "    * Calculate $\\nabla \\ell(\\theta)$ = = $X(Y - 1) + \\frac{exp(-\\theta X)X}{exp(-\\theta X)+1}$\n",
    "    * Update $\\theta \\leftarrow \\theta - \\alpha \\nabla\\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Write the pseudo-code for performing the stochastic gradient descent algorithm to solve the training of logistic regression problem (1). Please explain the difference between gradient descent and stochastic gradient descent for training logistic regression.\n",
    "\n",
    "* Given m examples of $(x^{(i)},y^{(i)})$, learning rate $\\alpha$\n",
    "* Randomly initialize **$\\theta$** , bias **b**\n",
    "* Repeat until convergence:\n",
    "    * For i = 1, 2, ..., m\n",
    "        * Predict $\\hat{y}^{(i)}$ = $h(x^{(i)})$ = z($\\theta x^{(i)} + b$)\n",
    "        * Calculate cost $\\ell(\\hat{y}^{(i)}, y^{(i)})$\n",
    "        * Calculate $\\nabla \\ell(\\theta)$ = $x^{(i)}(y^{(i)} - 1) + \\frac{exp(-\\theta x^{(i)})x^{(i)}}{exp(-\\theta x^{(i)})+1}$\n",
    "        * Update $\\theta \\leftarrow \\theta - \\alpha \\nabla\\ell(\\theta)$\n",
    "        \n",
    "        \n",
    "Gradient descent computes the gradients after iterating through all m training observations. With large training sets, this can become computationally expensive to do in the event of a vectorized implementation. Run time for an iterative approach can take a long time to loop through all observations as well. Stochastic gradient descent, on the other hand, either samples a small subset of traiing observations to compute the gradient. Or in the case of \"online\" learning, the gradient is calculated and the weights are updated after each observation seen (as shown above in the pseudo code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) We will show that the training problem in basic logistic regression problemis concave. Derive the Hessian matrix of (θ) and based on this, show the training problem (1) is concave (note that in this case, since we only have one feature, the Hessian matrixis just ascalar). Explain why the problem can be solved efficiently and gradient descent will achieve a unique global optimizer, as we discussed in class.\n",
    "\n",
    "Taking the derivative of the result from part **(a)** yields:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial'' \\ell}{\\partial \\theta''} = \\frac{\\partial' \\ell}{\\partial \\theta'} X(y-1) + \\frac{exp\\{-\\theta X\\}X}{exp\\{-\\theta X\\} + 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{= - \\frac{exp(-\\theta X)X^2}{(1 + exp(-\\theta X))^2}\\ \\ \\ \\ \\ \\ \\ \\text{(1)}}\n",
    "$$\n",
    "\n",
    "Since X $\\in \\mathbb{R}^{mx1}$ then there is a single theta in the Logistic Regression problem. Because of this, the hessian turns out to be a single value. In this instance, that value will be negative as seen above. Any real value entered in (1) above will be negative. The hessian of a function of a single variable is concave if its second derivative is negative everywhere. A concave optimization problem and conversely a convex optimization problem when minimzing an optjective function, will have a unique global solution and can be solved efficiently without fear of being stuck in a local minimum (or maximum). This is in contrast to a non-convex (non-concave) optimization problem, such as LP or integer optimization. These types of problems can get stuck in a local minimum and/or maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
