{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "## Linear regression: bias-variance tradeoff, CV, and variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider a dataset with n data points $(x^i , y^i ), x^i ∈ \\mathbb{R}^n$ , following from the following linear model:\n",
    "\n",
    "$$\n",
    "y^i = \\beta^{*^T} x^i + \\epsilon^i, \\ \\ \\ \\ \\ \\ \\text{i = 1, ...,m,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon^i$ are i.i.d. Gaussian noise with zero mean and variance $σ^2$ , and $β^∗$ is the true parameter.\n",
    "Consider the ridge regression as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}(\\lambda) = \\text{argmin}_{\\beta}{\\frac{1}{m} \\sum_{i=1}^{m}(y^i - \\beta^T x^i)^2 + \\lambda ||\\beta||_2^2}\n",
    "$$\n",
    "\n",
    "where $\\lambda \\leq 0$ is the regularized parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Find the closed form solution for $\\hat{\\beta}(\\lambda)$ and its distribution conditioning on ${x^i}$ (i.e., treat them as fixed).\n",
    "\n",
    "##### **Answer**\n",
    "Rewriting the equation into matrix form produces:\n",
    "\n",
    "$$\n",
    "|| y-X \\beta||_2^2 + \\lambda || \\beta ||_2^2\\ \\ \\ \\ (1)\n",
    "$$\n",
    "\n",
    "Taking the derivative of (1) with respect $\\beta$ and setting equal to 0 yields\n",
    "\n",
    "$$\n",
    "= (2y-2X\\beta)X + (2\\lambda\\beta) = 0\n",
    "\\\\\n",
    "= (2X^Ty - 2X^TX\\beta) + (2\\lambda\\beta) = 0\n",
    "\\\\\n",
    "= (X^Ty - X^TX\\beta) + (\\lambda\\beta) = 0\\ \\ \\ \\ (2)\n",
    "$$\n",
    "\n",
    "Solving (2) for $\\beta$ yields\n",
    "\n",
    "$$\n",
    "X^Ty = X^TX\\beta + \\lambda\\beta\n",
    "\\\\\n",
    "X^Ty = (X^TX + \\lambda I)\\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{(X^TX + \\lambda I)^{-1}X^Ty = \\beta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Calculate the bias $E[x^T β̂(λ)] − x^T β^∗$ as a function of λ and some fixed test point x.\n",
    "\n",
    "##### **Answer**\n",
    "\n",
    "Since y in the answer above is \n",
    "\n",
    "$$\n",
    "y = (X\\beta + \\epsilon)\n",
    "$$\n",
    "\n",
    "We can rewrite it as \n",
    "\n",
    "$$\n",
    "\\beta = (X^TX + \\lambda I)^{-1} X^T(X\\beta + \\epsilon)\n",
    "$$\n",
    "\n",
    "Expanding produces:\n",
    "$$\n",
    "(X^TX + \\lambda I)^{-1} X^TX\\beta + (X^TX + \\lambda I)^{-1} X^T \\epsilon\n",
    "$$\n",
    "\n",
    "Since we desire $E[\\beta]$ the equation above becomes\n",
    "\n",
    "$$\n",
    "(X^TX + \\lambda I)^{-1} X^TX\\beta + (X^TX + \\lambda I)^{-1} X^T E[\\epsilon]\n",
    "$$\n",
    "\n",
    "Since $\\epsilon$ is i.i.d with mean 0 and variance $\\sigma^2$, then $E[\\epsilon] = 0$ which reduces the above to\n",
    "\n",
    "$$\n",
    "(X^TX + \\lambda I)^{-1} X^TX\\beta\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "E[x^T\\beta(\\lambda)] - x^T\\beta^* \\\\\n",
    "= (X^TX + \\lambda I)^{-1} X^TX\\beta - x^T\\beta^*\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
