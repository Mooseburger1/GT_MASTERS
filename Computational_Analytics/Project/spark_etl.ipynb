{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "#! python -m spacy validate\n",
    "#! python -m spacy download en_core_web_sm\n",
    "# pip install spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sparknlp\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['get','from', 'subject', 're', 'edu', 'use', 'could', 'also', 'would', 'maybe', 'still', \n",
    "                   'say', 'go','be', 's','like', 'dont', 'dent', 'kind', 'maybe', 'didnt', 'went', 'wanted', \n",
    "                   'way', 'says', 'think', 'said', 'thats', 'thing', 'going', 'things', 'u', 'theres', 'cnn', 'fox', 'breitbart'])\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#import sys, glob, os\n",
    "#sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))\n",
    "\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adddd = ['get','from', 'subject', 're', 'edu', 'use', 'could', 'also', 'would', 'maybe', 'still', \n",
    "                   'say', 'go','be', 's','like', 'dont', 'dent', 'kind', 'maybe', 'didnt', 'went', 'wanted', \n",
    "                   'way', 'says', 'think', 'said', 'thats', 'thing', 'going', 'things', 'u', 'theres', 'cnn', 'fox', 'breitbart']\n",
    "\n",
    "for word in adddd:\n",
    "    stopWords.add(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'breitbart',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cnn',\n",
       " 'could',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'dent',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'dont',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'few',\n",
       " 'for',\n",
       " 'fox',\n",
       " 'from',\n",
       " 'further',\n",
       " 'get',\n",
       " 'go',\n",
       " 'going',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'kind',\n",
       " 'like',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'say',\n",
       " 'says',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'still',\n",
       " 'subject',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'theres',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'use',\n",
       " 've',\n",
       " 'very',\n",
       " 'wanted',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " 'went',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r'./data/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option(\"encoding\", \"UTF-8\").load(data, inferSchema='true', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql('''SELECT date,\n",
    "                         year,\n",
    "                         month,\n",
    "                         day,\n",
    "                         author,\n",
    "                         title,\n",
    "                         article,\n",
    "                         url,\n",
    "                         publication\n",
    "                   FROM articles \n",
    "                   WHERE article IS NOT NULL''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'date':'1970-01-01 00:00:00',\n",
    "                'year':1970,\n",
    "                'month':1.0,\n",
    "                'day':1.0,\n",
    "                'author':'missing',\n",
    "                'title':'missing',\n",
    "                'url':'missing',\n",
    "                'publication':'missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql('''\n",
    "                    SELECT *\n",
    "                    FROM articles\n",
    "                    WHERE year >= 2000\n",
    "                    AND year <2021\n",
    "\n",
    "               ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('article') \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['token']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemma')\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('clean_lemma') \\\n",
    "     .setCaseSensitive(False) \\\n",
    "     .setStopWords(stop_words)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['clean_lemma']) \\\n",
    "     .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           lemmatizer,\n",
    "           stopwords_cleaner,\n",
    "           finisher\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pipe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.createOrReplaceTempView('clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select year, finished_clean_lemma from clean limit 1').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ['date', 'year', 'month', 'day', 'author', 'title', 'article', 'url', 'publication']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = df.date.fillna('1970-01-01 00:00:00')\n",
    "df.year = df.year.fillna(1970)\n",
    "df.month = df.month.fillna(1.0)\n",
    "df.day = df.day.fillna(1.0)\n",
    "df.author = df.author.fillna('missing')\n",
    "df.title = df.title.fillna('missing')\n",
    "df.url = df.url.fillna('missing')\n",
    "df.publication = df.publication.fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'./data/pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argon2-cffi @ file:///tmp/build/80754af9/argon2-cffi_1596828496740/work\r\n",
      "async-generator==1.10\r\n",
      "attrs @ file:///tmp/build/80754af9/attrs_1600298409949/work\r\n",
      "backcall==0.2.0\r\n",
      "bleach @ file:///tmp/build/80754af9/bleach_1600439572647/work\r\n",
      "blis==0.4.1\r\n",
      "botocore==1.13.17\r\n",
      "catalogue==1.0.0\r\n",
      "certifi==2020.6.20\r\n",
      "cffi @ file:///tmp/build/80754af9/cffi_1600699165083/work\r\n",
      "chardet==3.0.4\r\n",
      "click==7.1.2\r\n",
      "colorama==0.4.1\r\n",
      "cymem==2.0.4\r\n",
      "decorator==4.4.2\r\n",
      "defusedxml==0.6.0\r\n",
      "docutils==0.15.2\r\n",
      "en-core-web-sm==2.2.0\r\n",
      "entrypoints==0.3\r\n",
      "idna==2.10\r\n",
      "importlib-metadata==2.0.0\r\n",
      "ipykernel @ file:///tmp/build/80754af9/ipykernel_1596206602906/work/dist/ipykernel-5.3.4-py3-none-any.whl\r\n",
      "ipython @ file:///tmp/build/80754af9/ipython_1593447367857/work\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets @ file:///tmp/build/80754af9/ipywidgets_1601490159889/work\r\n",
      "jedi @ file:///tmp/build/80754af9/jedi_1598371618777/work\r\n",
      "Jinja2==2.11.2\r\n",
      "jmespath==0.9.4\r\n",
      "joblib==0.17.0\r\n",
      "jsonschema @ file:///tmp/build/80754af9/jsonschema_1602607155483/work\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1601311786391/work\r\n",
      "jupyter-console @ file:///tmp/build/80754af9/jupyter_console_1598884538475/work\r\n",
      "jupyter-core==4.6.3\r\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\r\n",
      "MarkupSafe==1.1.1\r\n",
      "mistune==0.8.4\r\n",
      "murmurhash==1.0.4\r\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1602783176460/work\r\n",
      "nbconvert @ file:///tmp/build/80754af9/nbconvert_1601914804165/work\r\n",
      "nbformat @ file:///tmp/build/80754af9/nbformat_1602783287752/work\r\n",
      "nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1601499549014/work\r\n",
      "nltk==3.5\r\n",
      "notebook @ file:///tmp/build/80754af9/notebook_1601501581489/work\r\n",
      "numpy==1.19.4\r\n",
      "packaging==20.4\r\n",
      "pandas==1.1.4\r\n",
      "pandocfilters==1.4.2\r\n",
      "parso==0.7.0\r\n",
      "pexpect==4.8.0\r\n",
      "pickleshare==0.7.5\r\n",
      "plac==1.1.3\r\n",
      "preshed==3.0.3\r\n",
      "prometheus-client==0.8.0\r\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1602688806899/work\r\n",
      "ptyprocess==0.6.0\r\n",
      "py4j==0.10.7\r\n",
      "pyasn1==0.4.7\r\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\r\n",
      "Pygments @ file:///tmp/build/80754af9/pygments_1604103097372/work\r\n",
      "pyparsing==2.4.7\r\n",
      "pyrsistent @ file:///tmp/build/80754af9/pyrsistent_1600141725711/work\r\n",
      "pyspark==2.4.4\r\n",
      "python-dateutil==2.8.0\r\n",
      "pytz==2020.4\r\n",
      "PyYAML==5.1.2\r\n",
      "pyzmq==19.0.2\r\n",
      "qtconsole @ file:///tmp/build/80754af9/qtconsole_1600870028330/work\r\n",
      "QtPy==1.9.0\r\n",
      "regex==2020.10.28\r\n",
      "requests==2.24.0\r\n",
      "rsa==3.4.2\r\n",
      "s3transfer==0.2.1\r\n",
      "scott==1.7.2\r\n",
      "Send2Trash==1.5.0\r\n",
      "six==1.13.0\r\n",
      "spacy==2.3.2\r\n",
      "spark-nlp==2.6.3\r\n",
      "srsly==1.0.2\r\n",
      "terminado==0.9.1\r\n",
      "testpath==0.4.4\r\n",
      "thinc==7.4.1\r\n",
      "tornado==6.0.4\r\n",
      "tqdm==4.51.0\r\n",
      "traitlets==4.3.3\r\n",
      "urllib3==1.25.11\r\n",
      "wasabi==0.8.0\r\n",
      "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\r\n",
      "webencodings==0.5.1\r\n",
      "widgetsnbextension==3.5.1\r\n",
      "zipp==3.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
