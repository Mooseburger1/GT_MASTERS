{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1{-}\n",
    "## Density Estimation: Psychological Experiments{-}\n",
    "\n",
    "We will use this data to study whether or not the two brain regions are likely to be indepen-\n",
    "dent of each other and considering different types of political view For this question; you\n",
    "can use the proper package for histogram and KDE; no need to write your own.\n",
    "The data set n90pol.csv contains information on 90 university students who participated in\n",
    "a psychological experiment designed to look for relationships between the size of different\n",
    "regions of the brain and political views. The variables amygdala and acc indicate the volume\n",
    "of two particular brain regions known to be involved in emotions and decision-making, the\n",
    "amygdala and the anterior cingulate cortex; more exactly, these are residuals from the pre-\n",
    "dicted volume, after adjusting for height, sex, and similar body-type variables. The variable\n",
    "orientation gives the students’ locations on a five-point scale from 1 (very conservative) to 5\n",
    "(very liberal). Note that in the dataset, we only have observations for orientation from 2 to\n",
    "5.\n",
    "\n",
    "#### 1. Form the 1-dimensional histogram and KDE to estimate the distrbutions of amygdala and acc, respectively. For this question, you can ignore the variable **orientation**.{-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Answer** {-}\n",
    "![kds](kde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Form 2-dimensional histogram for the pairs of variables (amygdala, acc) Decide on a suitable number of bins so you can see the shape of the distribution clearly Also use kernel-density-estimation (KDE) to estimate the 2-dimensional density function of (amygdala, acc). Use a simple multi-dimensional Gaussian kernel, for {-}\n",
    "$$ x = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} \\in \\mathbb{R}^2$$\n",
    "\n",
    ", where $x_1$ and $x_2$ are the two dimensions respectively. \n",
    "$$K(x) = \\frac{1}{2\\pi} e^{-\\frac{(x_1)^2 + (x_2)^2}{2}}$$. \n",
    "Recall in this case, the kernel density estimator (KDE) for a density is given by. \n",
    "$$p(x) = \\frac{1}{m}\\sum_{i=1}^m\\frac{1}{h}K(\\frac{x^i - x}{h})$$\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "\n",
    "**2D Histogram**\n",
    "\n",
    "![2dhist](2d_hist.png)\n",
    "\n",
    "**2D KDE**\n",
    "\n",
    "![3d_hist](2d_kde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using (a) and (b), using KDE estimators, verify whether or not the variables amygdala and acc are independent? You can tell this by checking do we approximately have p(amygdala, acc) = p(amygdala)p(acc)? To verify this, please show three plots: the map for p(amygdala, acc), the map for p(amygdala)p(acc) and the error map |p(amygdala, acc)−p(amygdala)p(acc)|. Comment on your results and whether this helps us to find out whether the two parts of brains (for emotions and decision-making) functions independently or they are related. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "For the two features to be deemed independent, the error between the joint and the product of the marginal distrubtions must be approximately equal to 0. Although the shapes of the density estimates appear very similiar (graphs 1 and 2), we can see on the 3rd graph below that there is somewhat of a significant difference between the two types of density estimates. One could interpret this as the features not being independnet of each other. However, the error isn't of such a great magnitude that it emphatically confirms dependence.\n",
    "\n",
    "![3graph](3graph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4  Now we will consider the variable orientation. We will estimate the conditional distribution of the volume of the amygdala, conditioning on political orientation: p(amygdala|orientation = c), c = 2, . . . , 5. Do the same for the volume of the acc: Plot p(acc|orientation = c), c = 2, . . . , 5. You will use KDE to achieve the goal. (Note that the conditional distribution can be understood as fitting a distribution for the data with the same (fixed) orientation. Thus there should be 4 one-dimensional distribution functions to show for this question.) {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "\n",
    "\n",
    "![4kde](4kde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Again we will consider the variable orientation. We will estimate the conditional joint distribution of the volume of the amygdala and acc, conditioning on a function of political orientation: p(amygdala, acc|orientation = c), c = 2, . . . , 5. You will use two-dimensional KDE to achieve the goal.{-}\n",
    "\n",
    "##### **Answer**{-}\n",
    "\n",
    "\n",
    "\n",
    "![4ckde](4kde_conditional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using (d) and (e), evaluate whether or not the two variables are likely to be conditionally independent. To verify this, please show three plots: the map for p(amygdala, acc|orientation = c), the map for p(amygdala|orientation = c)p(acc|orientation = c) and the error map |p(amygdala, acc|orientation = c) − p(amygdala|orientation = c)p(acc|orientation = c)|, c = 2, . . . , 5. Comment on your results and whether this helps us to find out whether the two parts of brains (for emotions and decision-making) functions independently or they are related, conditionally on the political orientation (i.e., considering different types of personality). {-}\n",
    "\n",
    "##### Answer{-}\n",
    "\n",
    "**c=2**\n",
    "\n",
    "![2](condition2.png)\n",
    "\n",
    "**c=3**\n",
    "\n",
    "![3](condition3.png)\n",
    "\n",
    "**c=4**\n",
    "\n",
    "![4](condition4.png)\n",
    "\n",
    "**c=5**\n",
    "\n",
    "![5](condition5.png)\n",
    "\n",
    "\n",
    "Overall, there is significnat enough error amongst all conditional density estimtates, that it strengthens the argument that the two parts of the brain (amygdala, acc) are act in a dependent fashion of one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 {-}\n",
    "## Implementing EM for MNIST dataset, with PCA for dimensionality reduction. {-}\n",
    "\n",
    "Implement the EM algorithm for fitting a Gaussian mixture model for the MNIST dataset.\n",
    "We reduce the dataset to be only two cases, of digits “2” and “6” only. Thus, you will fit\n",
    "GMM with C = 2. Use the data file data.mat or data.dat. True label of the data are also\n",
    "provided in label.mat and label.dat\n",
    "The matrix images is of size 784-by-1990, i.e., there are totally 1990 images, and each\n",
    "column of the matrix corresponds to one image of size 28-by-28 pixels (the image is vectorized;\n",
    "the original image can be recovered by map the vector into a matrix).\n",
    "First use PCA to reduce the dimensionality of the data before applying to EM. We will\n",
    "put all “6” and “2” digits together, to project the original data into 5-dimensional vectors.\n",
    "Now implement EM algorithm for the projected data (with 5-dimensions).\n",
    "\n",
    "#### 1. Select from data one raw image of “2” and “6” and visualize them, respectively. {-}\n",
    "\n",
    "###### **Answer**\n",
    "\n",
    "![numbs](numbers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write down detailed expression of the E-step and M-step in the EM algorithm (hint: when computing $\\tau_k^i$ , you can drop the (2π) n/2 factor from the numerator and denominator expression, since it will be canceled out; this can help avoid some numerical issues in computation). {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "The data **X** in which the GMM is being fit on, contains 1990 observations, and is decomposed to its first 5 principal components. Therefore:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{1990 x 5}\n",
    "$$\n",
    "\n",
    "We are attmepting to fit 2 clusters, representative of the two images classes {2, 6}. Therefore:\n",
    "\n",
    "$$\n",
    "\\text{Number of Clusters K} = 2\n",
    "$$\n",
    "\n",
    "A single cluster **k**, which is a multivariate guassian $\\mathcal{N_k}(\\mu_k, \\Sigma_k)$, is parameterized by mean $\\mu_k$ and covariance matrix $\\Sigma_k$. Given the dimension of input data **X**:\n",
    "\n",
    "$$\n",
    "\\mathcal{N_k}(\\mu_k, \\Sigma_k) = \\frac{exp[\\frac{-1}{2}(X - \\mu_k)^T \\Sigma^{-1} (X-\\mu_k)]}{ \\sqrt{(2 \\pi)^n |\\Sigma|} }\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(1)}\n",
    "$$\n",
    "Where\n",
    "$$\n",
    "\\mu_k \\in \\mathbb{R}^{5 x 1}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_k \\in \\mathbb{R}^{5 x 5}\n",
    "$$\n",
    "\n",
    "\n",
    "**E-Step (Expectation)** {-}\n",
    "----------------\n",
    "The E-Step of the EM algorithm, aims to update the posterior probabilities for each observation with respect to each cluster in **K**. The posterior probability $\\tau_k^{(i)}$ is described by:\n",
    "$$\n",
    "\\tau_k^{(i)} = \\frac{ \\pi_k \\mathcal{N_k}(X^{(i)} | \\mu_k, \\Sigma_k) }{\\sum_{k=1}^{K=2} \\pi_k \\mathcal{N_k}(X^{(i)} | \\mu_k, \\Sigma_k) } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(2)}\n",
    "$$\n",
    "\n",
    "Where $\\pi_k$ is the mixing coefficient for clusteer **k**, $X^{(i)}$ is the $i^{th}$ observation in **X** and $\\tau_k^{(i)}$ is the posterior probability that $X^{(i)}$ comes from cluster **k**. Plugging (1) into (2) yields:\n",
    "\n",
    "$$\n",
    "\\tau_k^{(i)} = \\frac{ \\pi_k  \\frac{exp[\\frac{-1}{2}(X^{(i)} - \\mu_k)^T \\Sigma^{-1}_k (X^{(i)}-\\mu_k)]}{ \\sqrt{(2 \\pi)^n |\\Sigma_k|} }} {\\sum_{k=1}^{K=2} \\pi_k  \\frac{exp[\\frac{-1}{2}(X^{(i)} - \\mu_k)^T \\Sigma^{-1}_k (X^{(i)}-\\mu_k)]}{ \\sqrt{(2 \\pi)^n |\\Sigma_k|} } } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(3)}\n",
    "$$\n",
    "\n",
    "Extracting the constants from the numerator and denominator from (3)\n",
    "\n",
    "$$\n",
    "\\tau_k^{(i)} = \\frac{ \\frac{1}{\\sqrt{(2 \\pi)^n }} \\pi_k  \\frac{exp[\\frac{-1}{2}(X^{(i)} - \\mu_k)^T \\Sigma^{-1}_k (X^{(i)}-\\mu_k)]}{ \\sqrt{|\\Sigma_k|} }} { \\frac{1}{\\sqrt{(2 \\pi)^n}}  \\sum_{k=1}^{K=2} \\pi_k  \\frac{exp[\\frac{-1}{2}(X^{(i)} - \\mu_k)^T \\Sigma^{-1}_k (X^{(i)}-\\mu_k)]}{ \\sqrt{ |\\Sigma_k|} } } \n",
    "$$\n",
    "\n",
    "Cancelling out the constants yields\n",
    "\n",
    "$$\n",
    "\\boxed{\\tau_k^{(i)} = \\frac{  \\pi_k  \\frac{exp[\\frac{-1}{2}(X^{(i)} - \\mu_k)^T \\Sigma^{-1}_k (X^{(i)}-\\mu_k)]}{ \\sqrt{|\\Sigma_k|} }} {  \\sum_{k=1}^{K=2} \\pi_k  \\frac{exp[\\frac{-1}{2}(X^{(i)} - \\mu_k)^T \\Sigma^{-1}_k (X^{(i)}-\\mu_k)]}{ \\sqrt{ |\\Sigma_k|} } } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(4)}}\n",
    "$$\n",
    "\n",
    "**M-Step (Maximization)** {-}\n",
    "---------\n",
    "\n",
    "After calculating the posteriors for each cluster w.r.t to each obseravation in **X**, we update $\\pi_k$, $\\mu_k$ and $\\Sigma_k$ for each cluster with respect to the new densities corresponding to the posteriors calculated in the E-step. The mean for each cluster is updated by:\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_i \\tau_k^i X^i  }{ \\sum_i \\tau_i^k }\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(5)}\n",
    "$$\n",
    "for (i = 1, 2, 3, .... m). The mixing coefficients are updated by:\n",
    "$$\n",
    "\\pi_k = \\frac{ \\sum_i \\tau_k^i }{ m  }\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(6)}\n",
    "$$\n",
    "where m=1990 is the total number of observations in **X**. Finally the covariances for each cluster are updated by:\n",
    "$$\n",
    "\\Sigma_k = \\frac{ \\sum_i \\tau_k^i (X^i - \\mu_k)(X^i - \\mu_k)^T }{\\sum_i \\tau_k^i  } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(7)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implement EM algorithm yourself. Use the following initialization {-}\n",
    "* initialization for mean: random Gaussian vector with zero mean\n",
    "* initialization for covariance: generate two Gaussian random matrix of size n-by- n: $S_1$ and $S_2$ , and initialize the covariance matrix for the two components are $\\Sigma_1 = S_1S_1^T + I_n$ , and $\\Sigma_2 = S_2S_2^T + I_n$ , where $I_n$ is an identity matrix of size n-by-n.\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "```\n",
    "class GMM:\n",
    "    def __init__(self, K: int, maxIter: int, tol=1e-3, seed=42, savefig=None) -> None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        #Number of gaussian clusters\n",
    "        self.K = K\n",
    "        self.maxIter = maxIter\n",
    "        self.tol = tol\n",
    "        self.lls = []\n",
    "        self.save = savefig\n",
    "        \n",
    "    def fit(self, X: np.array):\n",
    "        \n",
    "        assert len(X.shape) >= 2, 'If passing a vector of data, ensure two dimensions by reshaping data \"X.reshape(-1,1)\"'\n",
    "        #initialize clusters\n",
    "        self._initialize(X=X)\n",
    "        \n",
    "        for i in range(self.maxIter):\n",
    "            \n",
    "            print('.........iteration {}.........'.format(i))\n",
    "            #run the E step\n",
    "            self._expectation(X)\n",
    "            \n",
    "            #run the M step\n",
    "            self._maximization(X)\n",
    "            \n",
    "            diff = np.linalg.norm(self.mus - self.mus_old)\n",
    "\n",
    "            if np.linalg.norm(self.mus-self.mus_old) < self.tol:\n",
    "                print('Converged!')\n",
    "                break\n",
    "            \n",
    "            self.mus_old = self.mus.copy()\n",
    "        \n",
    "        if i == self.maxIter-1:\n",
    "            print('Max iterations reached')\n",
    "            \n",
    "        plt.figure(figsize=(25,10))\n",
    "        plt.plot(self.lls, ls='--', lw=3)\n",
    "        plt.scatter(range(len(self.lls)), self.lls, marker='o', edgecolor='k', s=200, color='gray',zorder=3)\n",
    "        plt.grid()\n",
    "        plt.xlabel('Iterations', fontsize=20)\n",
    "        plt.ylabel('Log Likelihood', fontsize=20)\n",
    "        if self.save:\n",
    "            plt.savefig(self.save, bbox_layout='tight', bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _initialize(self, X):\n",
    "        #get shape of the data\n",
    "        m,n = X.shape\n",
    "        \n",
    "        #initialize K-gaussian clusters with mean randomly sampled from 0 centered Gaussian\n",
    "        self.mus = np.random.randn(self.K, n)\n",
    "        self.mus_old = self.mus.copy()\n",
    "        \n",
    "        #initialize K-covariance matrices\n",
    "        self.covs = []\n",
    "        for _ in range(self.K):\n",
    "            S = np.random.randn(n,n)\n",
    "            S = S@S.T + np.eye(n)\n",
    "            self.covs.append(S)\n",
    "\n",
    "        \n",
    "        #initialize priors\n",
    "        self.pis = np.random.random(self.K)\n",
    "        self.pis = self.pis / np.sum(self.pis)\n",
    "        \n",
    "        #initialize the posterior\n",
    "        self.tau = np.full((m,self.K), fill_value=0.)\n",
    "        \n",
    "    def _expectation(self, X: np.array):\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            self.tau[:, i] = self.pis[i] * multivariate_normal.pdf(X, \n",
    "                                                                   self.mus[i], \n",
    "                                                                   self.covs[i])\n",
    "        #normalize tau\n",
    "        sum_tau = np.sum(self.tau, axis=1).reshape(-1,1)\n",
    "        self.tau = np.divide(self.tau, np.tile(sum_tau, (1,self.K)))\n",
    "        \n",
    "        #calculate log-likelihood\n",
    "        ll = np.sum(np.log(sum_tau))\n",
    "        self.lls.append(ll)\n",
    "        \n",
    "    def _maximization(self, X: np.array):\n",
    "        \n",
    "        m,n = X.shape\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            \n",
    "            #update priors\n",
    "            self.pis[i] = np.sum(self.tau[:,i]) / m\n",
    "            \n",
    "            #update cluster mean\n",
    "            self.mus[i] = X.T @ self.tau[:, i] / np.sum(self.tau[:,i], axis=0)\n",
    "            \n",
    "            #update cluster covariance\n",
    "            dummy = X - np.tile(self.mus[i], (m,1))\n",
    "            self.covs[i] = dummy.T @ np.diag(self.tau[:, i]) @ dummy / np.sum(self.tau[:,i], axis=0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Report, the fitted GMM model when EM has terminated in your algorithms as follows. Make sure to report the weights for each component, and the mean of each component (you can reformat the vector to make them into 28-by-28 matrices and show images). Ideally, you should be able to see these means corresponds to “average” images. Report the two 784-by-784 covariance matrices by visualize their intensities. {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "**Log Likelihood / Iterations**\n",
    "\n",
    "\n",
    "![ll](ll.png)\n",
    "\n",
    "\n",
    "![means](means.png)\n",
    "\n",
    "\n",
    "![cov](covs.png)\n",
    "\n",
    "**Cluster Weights ($\\pi_k$)**\n",
    "\n",
    "| Cluster (K) | Weights ($\\pi_k$) |\n",
    "|-------------|--------------|\n",
    "| 1           | 0.5068       |\n",
    "| 2           | 0.4931       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Use the $\\tau_k^i$ to infer the labels of the images, and compare with the true labels. Report the mis-classification rate for digits “2” and “6” respectively. Perform K-means clustering with K = 2 (you may call a package or use the code from your previous homework). Find out the mis-classification rate for digits “2” and “6” respectively, and compare with GMM. Which one achieves the better performance? {-}\n",
    "\n",
    "##### **Answer** {-}\n",
    "\n",
    "**GMM**\n",
    "\n",
    "Cluster 1: Image of Number 6\n",
    "Cluster 2: Image of Number 2\n",
    "\n",
    "\n",
    "| Cluster 1              |            |\n",
    "|------------------------|------------|\n",
    "| 2's                      | 60 points  |\n",
    "| 6's                     | 951 points |\n",
    "| Misclassification rate | 5.93%      |\n",
    "\n",
    "\n",
    "| Cluster 2              |            |\n",
    "|------------------------|------------|\n",
    "| 2's                      | 972 points |\n",
    "| 6's                     | 7 points   |\n",
    "| Misclassification rate | 0.72%      |\n",
    "\n",
    "\n",
    "| GMM                   |       |\n",
    "|-----------------------|-------|\n",
    "| Overall Inaccuracy    | 3.37% |\n",
    "| Model Imprecision (2's) | 5.81% |\n",
    "| Model Imprecision (6's) | 0.73% |\n",
    "\n",
    "\n",
    "**KMeans**\n",
    "\n",
    "Cluster 1: Image of Number 6\n",
    "Cluster 2: Image of Number 2\n",
    "\n",
    "| Cluster 1              |            |\n",
    "|------------------------|------------|\n",
    "| 2's                      | 79 points  |\n",
    "| 6's                     | 915 points |\n",
    "| Misclassification rate | 5.93%      |\n",
    "\n",
    "\n",
    "| Cluster 2              |            |\n",
    "|------------------------|------------|\n",
    "| 2's                      | 953 points |\n",
    "| 6's                     | 43 points   |\n",
    "| Misclassification rate | 4.32%      |\n",
    "\n",
    "\n",
    "| KMeans                |       |\n",
    "|-----------------------|-------|\n",
    "| Overall Inaccuracy    | 6.13% |\n",
    "| Model Imprecision (2) | 7.66% |\n",
    "| Model Imprecision (6) | 4.49% |\n",
    "\n",
    "\n",
    "Overall GMM performs better than KMeans. This is especially True w.r.t to classifying images of the number 2. GMM incorrectly grouped seven images of 6's in the cluster of 2's. Conversely KMeans incorrectly grouped 43 images of 6's in the cluster of 2's. GMM's overall accuracy 96.63% compared to KMeans 93.87%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
