{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Churn.csv')\n",
    "df.head()\n",
    "X = df.iloc[:, :-1].values.tolist()\n",
    "y = df.iloc[:, -1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[2.771244718,1.784783929],\n",
    "\t[1.728571309,1.169761413],\n",
    "\t[3.678319846,2.81281357],\n",
    "\t[3.961043357,2.61995032],\n",
    "\t[2.999208922,2.209014212],\n",
    "\t[7.497545867,3.162953546],\n",
    "\t[9.00220326,3.339047188],\n",
    "\t[7.444542326,0.476683375],\n",
    "\t[10.12493903,3.234550982],\n",
    "\t[6.642287351,3.319983761]]\n",
    "\n",
    "y = [0,0,0,0,0,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# This method computes entropy for information gain\n",
    "def entropy(class_y):\n",
    "    # Input:            \n",
    "    #   class_y         : list of class labels (0's and 1's)\n",
    "    \n",
    "    # TODO: Compute the entropy for a list of classes\n",
    "    #\n",
    "    # Example:\n",
    "    #    entropy([0,0,0,1,1,1,1,1,1]) = 0.92\n",
    "        \n",
    "    entropy = 0\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "        \n",
    "    unique_vals, counts = np.unique(class_y, return_counts=True)\n",
    "    tot = np.sum(counts)\n",
    "\n",
    "    probs = {x[0]:x[1]/tot for x in zip(unique_vals,counts)}\n",
    "\n",
    "    entropy = np.sum([-(val * np.log2(val)) for val in probs.values()])\n",
    "    #############################################\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    # Inputs:\n",
    "    #   X               : data containing all attributes\n",
    "    #   y               : labels\n",
    "    #   split_attribute : column index of the attribute to split on\n",
    "    #   split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    # TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    # \n",
    "    # You will have to first check if the split attribute is numerical or categorical    \n",
    "    # If the split attribute is numeric, split_val should be a numerical value\n",
    "    # For example, your split_val could be the mean of the values of split_attribute\n",
    "    # If the split attribute is categorical, split_val should be one of the categories.   \n",
    "    #\n",
    "    # You can perform the partition in the following way\n",
    "    # Numeric Split Attribute:\n",
    "    #   Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "    #   the rows where the split attribute is less than or equal to the split value, and the \n",
    "    #   second list has all the rows where the split attribute is greater than the split \n",
    "    #   value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    #\n",
    "    # Categorical Split Attribute:\n",
    "    #   Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "    #   the rows where the split attribute is equal to the split value, and the second list\n",
    "    #   has all the rows where the split attribute is not equal to the split value.\n",
    "    #   Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "    '''\n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 and split_val = 3 (mean of column 0)\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3  and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "    ''' \n",
    "    \n",
    "    X_left = []\n",
    "    X_right = []\n",
    "    \n",
    "    y_left = []\n",
    "    y_right = []\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "    \n",
    "    #check if split attribute is numeric\n",
    "    if isinstance(split_val, (float,int)):\n",
    "        assert isinstance(X[0][split_attribute], (float,int)), 'split_val is a numerical criteria, yet a test of the data type on one of the values in the specified column is not numeric'\n",
    "\n",
    "        #split the data on the specified column (split_attribute) by the specified criteria (split_val)\n",
    "        _ = [(X_left.append(row), y_left.append(y[pos])) if row[split_attribute] <= split_val else (X_right.append(row), y_right.append(y[pos])) for pos,row in enumerate(X)]\n",
    "\n",
    "    \n",
    "    #check if split attribute is categorical\n",
    "    elif isinstance(split_val, str):\n",
    "        assert isinstance(X[0][split_attribute], str), 'split_val is a categorical criteria, yet a test of the data type on one of the values in the specified column is not categorical'\n",
    "\n",
    "        #split the data on the specified column (split_attribute) by the specified criteria (split_val)\n",
    "        _ = [(X_left.append(row), y_left.append(y[pos])) if row[split_attribute] == split_val else (X_right.append(row), y_right.append(y[pos])) for pos,row in enumerate(X)]\n",
    "\n",
    "    #No clue what they did\n",
    "    else:\n",
    "        raise ValueError('Bad arguments for method partition_classes')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    return (X_left, X_right, y_left, y_right)\n",
    "\n",
    "    \n",
    "def information_gain(previous_y, current_y):\n",
    "    # Inputs:\n",
    "    #   previous_y: the distribution of original labels (0's and 1's)\n",
    "    #   current_y:  the distribution of labels after splitting based on a particular\n",
    "    #               split attribute and split value\n",
    "    \n",
    "    # TODO: Compute and return the information gain from partitioning the previous_y labels\n",
    "    # into the current_y labels.\n",
    "    # You will need to use the entropy function above to compute information gain\n",
    "    # Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    Example:\n",
    "    \n",
    "    previous_y = [0,0,0,1,1,1]\n",
    "    current_y = [[0,0], [1,1,1,0]]\n",
    "    \n",
    "    info_gain = 0.45915\n",
    "    \"\"\"\n",
    "\n",
    "    info_gain = 0\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "        \n",
    "    # H = entropy of original set \n",
    "    H = entropy(previous_y)\n",
    "\n",
    "    # HL = entropy of left split\n",
    "    HL = entropy(current_y[0])\n",
    "\n",
    "    # PL = # of obs in the left split / count(previous_y)\n",
    "    PL = len(current_y[0]) / len(previous_y)\n",
    "\n",
    "    # HR = entropy of right split\n",
    "    HR = entropy(current_y[1])\n",
    "\n",
    "    #PR = # of obs in the right split / count(previous_y)\n",
    "    PR = len(current_y[1]) / len(previous_y)\n",
    "\n",
    "    info_gain = H - (HL * PL + HR * PR)\n",
    "    #############################################\n",
    "    return info_gain\n",
    "    \n",
    "    \n",
    "def best_split(X, y, random_=True):\n",
    "    # Inputs:\n",
    "    #   X       : Data containing all attributes\n",
    "    #   y       : labels\n",
    "    # TODO: For each node find the best split criteria and return the \n",
    "    # split attribute, spliting value along with \n",
    "    # X_left, X_right, y_left, y_right (using partition_classes)\n",
    "    '''\n",
    "    \n",
    "    NOTE: Just like taught in class, don't use all the features for a node.\n",
    "    Repeat the steps:\n",
    "\n",
    "    1. Select m attributes out of d available attributes\n",
    "    2. Pick the best variable/split-point among the m attributes\n",
    "    3. return the split attributes, split point, left and right children nodes data \n",
    "\n",
    "    '''\n",
    "    split_attribute = 0\n",
    "    split_value = 0\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "    \n",
    "    #infomration gain value\n",
    "    ig = -np.inf\n",
    "\n",
    "    # Num of attributes\n",
    "    d = len(X[0])\n",
    "\n",
    "    # set m attributes\n",
    "    if random_:\n",
    "        m = int(d/2)\n",
    "        if m == 0:\n",
    "            m = 1\n",
    "    else:\n",
    "        m = d\n",
    "    # choice of m attributes out of d available attributes\n",
    "    col_idxs = np.random.choice(a=range(d), size=m, replace=False).tolist()\n",
    "\n",
    "    #subset the data on the randomly chosen columns\n",
    "    X_subset = [[row[i] for i in col_idxs] for row in X]\n",
    "\n",
    "    #loop through each column and find best split\n",
    "    for pos, col in enumerate(col_idxs):\n",
    "        #process logic for categorical col\n",
    "        if isinstance(X_subset[0][pos], str):\n",
    "            #get the unique categories\n",
    "            categories = np.unique([row[pos] for row in X_subset])\n",
    "            #loop through each category type and gauge split on it\n",
    "            for cat in categories:\n",
    "                #split on category\n",
    "                xl, xr, yl, yr = partition_classes(X=X_subset, y=y, split_attribute=pos, split_val=cat)\n",
    "                #information gain\n",
    "                new_ig = information_gain(previous_y=y, current_y=[yl, yr])\n",
    "                #check if ig is better\n",
    "                if new_ig > ig:\n",
    "                    ig = new_ig\n",
    "                    split_attribute = col\n",
    "                    split_value = cat\n",
    "                    X_left = xl\n",
    "                    X_right = xr\n",
    "                    y_left = yl\n",
    "                    y_right = yr\n",
    "\n",
    "        #process logic for continuous col\n",
    "        if isinstance(X_subset[0][pos], (float,int)):\n",
    "            #calculate the median\n",
    "            criteria_val = np.median([row[pos] for row in X_subset])\n",
    "            #split on criteria val\n",
    "            xl, xr, yl, yr = partition_classes(X=X_subset, y=y, split_attribute=pos, split_val=criteria_val)\n",
    "            #information gain\n",
    "            new_ig = information_gain(previous_y=y, current_y=[yl, yr])\n",
    "            #check if ig is better\n",
    "            if new_ig > ig:\n",
    "                ig = new_ig\n",
    "                split_attribute = col\n",
    "                split_value = criteria_val\n",
    "                X_left = xl\n",
    "                X_right = xr\n",
    "                y_left = yl\n",
    "                y_right = yr\n",
    "\n",
    "    return (X_left, X_right, y_left, y_right, split_attribute, split_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #############################################\n",
    "\n",
    "    \n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth, min_size=5):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        self.tree = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "    \n",
    "    def learn(self, X, y, par_node = {}, depth=0):\n",
    "        # TODO: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in utils.py to train the tree\n",
    "\n",
    "        # Use the function best_split in util.py to get the best split and \n",
    "        # data corresponding to left and right child nodes\n",
    "        \n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a \n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        #set the root of the tree\n",
    "\n",
    "        \n",
    "\n",
    "        # processing for starting at the stump\n",
    "        if len(par_node) == 0:\n",
    "            #find best split of the stump\n",
    "            (X_left, X_right, y_left, y_right, split_attribute, split_value) = best_split(X, y)\n",
    "            \n",
    "\n",
    "            self.tree = { 'depth': 1,\n",
    "                          'split_attr': split_attribute,\n",
    "                          'split_value': split_value,\n",
    "                          'left': (X_left, y_left),\n",
    "                          'right': (X_right, y_right)\n",
    "                          \n",
    "                        }\n",
    "            self.learn(X = _, y = _, par_node = self.tree, depth=depth)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            # [0] = X-vals\n",
    "            # [1] = y-vals\n",
    "            left, right = par_node['left'] , par_node['right']\n",
    "\n",
    "\n",
    "            #No split\n",
    "#             if len(left[0]) == 0 or len(right[0]) == 0:\n",
    "#                 print('\\n\\nTHIS IS LEFT', left)\n",
    "#                 print('\\n\\nTHIS IS RIGHT', right)\n",
    "#                 pprint.pprint(self.tree)\n",
    "#                 par_node['left'] = par_node['right'] = self._make_terminal( par_node['left'][1] + par_node['right'][1])\n",
    "                \n",
    "\n",
    "\n",
    "            #check max depth constraint\n",
    "            if par_node['depth'] >= self.max_depth:\n",
    "                par_node['left'], par_node['right'] = self._make_terminal(par_node['left'][1]) , self._make_terminal(par_node['right'][1])\n",
    "                \n",
    "\n",
    "\n",
    "            # *_split[0] = X_left\n",
    "            # *_split[1] = X_right\n",
    "            # *_split[2] = y_left\n",
    "            # *_split[3] = y_right\n",
    "            # *_split[4] = split_attribute\n",
    "            # *_splti[5] = split_value\n",
    "           \n",
    "            \n",
    "\n",
    "            #process left child - check if pure already\n",
    "            if all(ele == left[1][0] for ele in left[1]):\n",
    "                par_node['left'] = self._make_terminal(par_node['left'][1])\n",
    "                \n",
    "            elif len(left[1]) <= self.min_size:\n",
    "                par_node['left'] = self._make_terminal(par_node['left'][1])\n",
    "  \n",
    "            #check if depth has been met\n",
    "            elif par_node['depth'] >= depth:\n",
    "                par_node['left'] = self._make_terminal(par_node['left'][1])\n",
    "            \n",
    "            else:\n",
    "                left_split = best_split(*left, random_=True)\n",
    "                \n",
    "                left_node = { 'depth': par_node['depth'] + 1,\n",
    "                              'split_attr': left_split[4],\n",
    "                              'split_value': left_split[5],\n",
    "                              'left': (left_split[0], left_split[2]),\n",
    "                              'right': (left_split[1], left_split[3]),\n",
    "                            }\n",
    "                par_node['left'] = left_node\n",
    "                self.learn(X=_, y=_, par_node=par_node['left'], depth=depth)\n",
    "\n",
    "\n",
    "\n",
    "            #prcess right child - check if pure already\n",
    "            if all(ele == right[1][0] for ele in right[1]):\n",
    "                par_node['right'] = self._make_terminal(par_node['right'][1])\n",
    "                \n",
    "            elif len(right[1]) <= self.min_size:\n",
    "                par_node['right'] = self._make_terminal(par_node['right'][1])\n",
    "\n",
    "            #check if depth has been met\n",
    "            elif par_node['depth'] >= depth:\n",
    "                par_node['right'] = self._make_terminal(par_node['right'][1])\n",
    "\n",
    "            else:\n",
    "                right_split = best_split(*right, random_=True)\n",
    "                right_node = { 'depth': par_node['depth'] + 1,\n",
    "                               'split_attr': right_split[4],\n",
    "                               'split_value': right_split[5],\n",
    "                               'left': (right_split[0], right_split[2]),\n",
    "                               'right': (right_split[1], right_split[3]),\n",
    "                            }\n",
    "                par_node['right'] = right_node\n",
    "                self.learn(X=_, y=_, par_node=par_node['right'], depth=depth)\n",
    "            \n",
    "        #############################################\n",
    "    def _make_terminal(self, y):\n",
    "        return max(set(y), key=y.count)\n",
    "    \n",
    "    \n",
    "    def classify(self, record, par_node=None):\n",
    "        # TODO: classify the record using self.tree and return the predicted label\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        if par_node is None:\n",
    "            par_node = self.tree\n",
    "    \n",
    "        if record[par_node['split_attr']] < par_node['split_value']:\n",
    "            if isinstance(par_node['left'], dict):\n",
    "                return self.classify(record, par_node['left'])\n",
    "            else:\n",
    "                return par_node['left']\n",
    "        else:\n",
    "            if isinstance(par_node['right'], dict):\n",
    "                return self.classify(record, par_node['right'])\n",
    "            else:\n",
    "                return par_node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree(max_depth=100, min_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.learn(X=X, y=y, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'depth': 1,\n",
      " 'left': {'depth': 2,\n",
      "          'left': 0,\n",
      "          'right': 0,\n",
      "          'split_attr': 0,\n",
      "          'split_value': 1.784783929},\n",
      " 'right': {'depth': 2,\n",
      "           'left': 1,\n",
      "           'right': 1,\n",
      "           'split_attr': 0,\n",
      "           'split_value': 3.234550982},\n",
      " 'split_attr': 1,\n",
      " 'split_value': 2.716381945}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(tree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  0 \ty-true:  0 \tCORRECT!\n",
      "Prediction:  0 \ty-true:  0 \tCORRECT!\n",
      "Prediction:  1 \ty-true:  0 \tNOT CORRECT!\n",
      "Prediction:  0 \ty-true:  0 \tCORRECT!\n",
      "Prediction:  0 \ty-true:  0 \tCORRECT!\n",
      "Prediction:  1 \ty-true:  1 \tCORRECT!\n",
      "Prediction:  1 \ty-true:  1 \tCORRECT!\n",
      "Prediction:  0 \ty-true:  1 \tNOT CORRECT!\n",
      "Prediction:  1 \ty-true:  1 \tCORRECT!\n",
      "Prediction:  1 \ty-true:  1 \tCORRECT!\n"
     ]
    }
   ],
   "source": [
    "for pos,row in enumerate(X):\n",
    "    \n",
    "    pred = tree.classify(row)\n",
    "    y_true = y[pos]\n",
    "    \n",
    "    if pred == y_true:\n",
    "        print('Prediction: ', pred,'\\ty-true: ', y[pos],'\\tCORRECT!')\n",
    "    else:\n",
    "        print('Prediction: ', pred,'\\ty-true: ', y[pos],'\\tNOT CORRECT!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_node['left'] = par_node['right'] = self._make_terminal( par_node['left'][1] + par_node['right'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda496e4f14cb09440c9a1e4075475d115c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
