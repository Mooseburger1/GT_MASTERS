{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from code.mydatasets import load_seizure_dataset\n",
    "from code.utils import train, evaluate\n",
    "from code.plots import plot_learning_curves, plot_confusion_matrix\n",
    "from code.mymodels import MyMLP, MyCNN, MyRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a correct path to the seizure data file you downloaded\n",
    "PATH_TRAIN_FILE = \"data/seizure/seizure_train.csv\"\n",
    "PATH_VALID_FILE = \"data/seizure/seizure_validation.csv\"\n",
    "PATH_TEST_FILE = \"data/seizure/seizure_test.csv\"\n",
    "\n",
    "# Path for saving model\n",
    "PATH_OUTPUT = \"output/seizure/\"\n",
    "os.makedirs(PATH_OUTPUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "MODEL_TYPE = 'MLP'  # TODO: Change this to 'MLP', 'CNN', or 'RNN' according to your task\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "USE_CUDA = False  # Set 'True' if you want to use GPU\n",
    "NUM_WORKERS = 0  # Number of threads used by DataLoader. You can adjust this according to your machine spec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if USE_CUDA and torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(1)\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_seizure_dataset(PATH_TRAIN_FILE, MODEL_TYPE)\n",
    "valid_dataset = load_seizure_dataset(PATH_VALID_FILE, MODEL_TYPE)\n",
    "test_dataset = load_seizure_dataset(PATH_TEST_FILE, MODEL_TYPE)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == 'MLP':\n",
    "    model = MyMLP()\n",
    "    save_file = 'MyMLP.pth'\n",
    "elif MODEL_TYPE == 'CNN':\n",
    "    model = MyCNN()\n",
    "    save_file = 'MyCNN.pth'\n",
    "elif MODEL_TYPE == 'RNN':\n",
    "    model = MyRNN()\n",
    "    save_file = 'MyRNN.pth'\n",
    "else:\n",
    "    raise AssertionError(\"Wrong Model Type!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "train_losses, train_accuracies = [], []\n",
    "valid_losses, valid_accuracies = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    valid_loss, valid_accuracy, valid_results = evaluate(model, device, valid_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "    is_best = valid_accuracy > best_val_acc  # let's keep the model that has the best accuracy, but you can also use another metric.\n",
    "    if is_best:\n",
    "        best_val_acc = valid_accuracy\n",
    "        torch.save(model, os.path.join(PATH_OUTPUT, save_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(train_losses, valid_losses, train_accuracies, valid_accuracies)\n",
    "\n",
    "best_model = torch.load(os.path.join(PATH_OUTPUT, save_file))\n",
    "test_loss, test_accuracy, test_results = evaluate(best_model, device, test_loader, criterion)\n",
    "\n",
    "class_names = ['Seizure', 'TumorArea', 'HealthyArea', 'EyesClosed', 'EyesOpen']\n",
    "plot_confusion_matrix(test_results, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_TRAIN_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y = df.y.apply(lambda x: x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        \n",
    "        #initialize the architecture of the MLP\n",
    "        #One hidden layer with 16 nodes\n",
    "        self.hidden = nn.Linear(178, 16, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.output = nn.Linear(16, 5, bias = True)\n",
    "    def forward(self, x):\n",
    "        h1_linear = self.hidden(x)\n",
    "        h1_activation = self.sigmoid(linear)\n",
    "        output = self.output(h1_activation)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X).double().unsqueeze(2)\n",
    "train_y = torch.from_numpy(y).double()\n",
    "dataset = TensorDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = MyMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(ds_loader)\n",
    "X_samples, y_samples = dataiter.next()\n",
    "\n",
    "print(X_samples.shape)\n",
    "print(y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 16]           2,864\n",
      "           Sigmoid-2                [-1, 1, 16]               0\n",
      "            Linear-3                 [-1, 1, 5]              85\n",
      "================================================================\n",
      "Total params: 2,949\n",
      "Trainable params: 2,949\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(testModel, (1,178))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyRNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "\n",
    "flops, params = get_model_complexity_info(model, ( 1, 178), as_strings=True, print_per_layer_stat=True)\n",
    "print('Flops:  ' + flops)\n",
    "print('Params: ' + params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "##### DO NOT MODIFY OR REMOVE THIS VALUE #####\n",
    "checksum = '169a9820bbc999009327026c9d76bcf1'\n",
    "##### DO NOT MODIFY OR REMOVE THIS VALUE #####\n",
    "\n",
    "PATH_TRAIN = \"data/mortality/train/\"\n",
    "PATH_VALIDATION = \"../data/mortality/validation/\"\n",
    "PATH_TEST = \"../data/mortality/test/\"\n",
    "PATH_OUTPUT = \"../data/mortality/processed/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_icd9(icd9_object):\n",
    "\n",
    "    icd9_str = str(icd9_object)\n",
    "\t# TODO: Extract the the first 3 or 4 alphanumeric digits prior to the decimal point from a given ICD-9 code.\n",
    "\t# TODO: Read the homework description carefully.\n",
    "    if icd9_str.lower().startswith('e'): return icd9_str.split('.')[0][:4]\n",
    "    else: return icd9_str.split('.')[0][:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame(['V1046', 'V090', '135', '1890', '19889', 'E9352', 'E935'], columns=['ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = example.ICD9_CODE.apply(convert_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_codemap(df_icd9, transform):\n",
    "\t\"\"\"\n",
    "\t:return: Dict of code map {main-digits of ICD9: unique feature ID}\n",
    "\t\"\"\"\n",
    "\t# TODO: We build a code map using ONLY train data. Think about how to construct validation/test sets using this.\n",
    "\tdf_digits = df_icd9['ICD9_CODE'].apply(transform)\n",
    "\n",
    "\t\n",
    "    \n",
    "    \n",
    "\treturn {str(x):pos for pos,x in enumerate(df_digits.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, codemap, transform):\n",
    "\t\"\"\"\n",
    "\t:param path: path to the directory contains raw files.\n",
    "\t:param codemap: 3-digit ICD-9 code feature map\n",
    "\t:param transform: e.g. convert_icd9\n",
    "\t:return: List(patient IDs), List(labels), Visit sequence data as a List of List of List.\n",
    "\t\"\"\"\n",
    "\t# TODO: 1. Load data from the three csv files\n",
    "\t# TODO: Loading the mortality file is shown as an example below. Load two other files also.\n",
    "\tdf_mortality = pd.read_csv(os.path.join(path, \"MORTALITY.csv\"))\n",
    "\n",
    "\t# TODO: 2. Convert diagnosis code in to unique feature ID.\n",
    "\t# TODO: HINT - use 'transform(convert_icd9)' you implemented and 'codemap'.\n",
    "\n",
    "\t# TODO: 3. Group the diagnosis codes for the same visit.\n",
    "\n",
    "\t# TODO: 4. Group the visits for the same patient.\n",
    "\n",
    "\t# TODO: 5. Make a visit sequence dataset as a List of patient Lists of visit Lists\n",
    "\t# TODO: Visits for each patient must be sorted in chronological order.\n",
    "\n",
    "\t# TODO: 6. Make patient-id List and label List also.\n",
    "\t# TODO: The order of patients in the three List output must be consistent.\n",
    "\tpatient_ids = [0, 1, 2]\n",
    "\tlabels = [1, 0, 1]\n",
    "\tseq_data = [[[0, 1], [2]], [[1, 3, 4], [2, 5]], [[3], [5]]]\n",
    "\treturn patient_ids, labels, seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortality = pd.read_csv(os.path.join(PATH_TRAIN, \"MORTALITY.csv\"))\n",
    "df_diagnosis = pd.read_csv(os.path.join(PATH_TRAIN, \"DIAGNOSES_ICD.csv\"))\n",
    "df_admissions = pd.read_csv(os.path.join(PATH_TRAIN, \"ADMISSIONS.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnosis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codemap = build_codemap(df_diagnosis, convert_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnosis.ICD9_CODE = df_diagnosis.ICD9_CODE.apply(convert_icd9).apply(lambda x: codemap[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnosis.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions[df_admissions.HADM_ID == 172335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_admiss = pd.merge(left=df_diagnosis.loc[:,['SUBJECT_ID','HADM_ID','ICD9_CODE']], right=df_admissions.loc[:,['SUBJECT_ID','HADM_ID','ADMITTIME']], how='inner', on=['SUBJECT_ID', 'HADM_ID']).drop('HADM_ID', axis=1).sort_values(['SUBJECT_ID', 'ADMITTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_admiss.groupby(['SUBJECT_ID','ADMITTIME'])['ICD9_CODE'].apply(list).reset_index().groupby('SUBJECT_ID')['ICD9_CODE'].apply(list).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_TRAIN_SEQS = os.path.join(project_root, \"data/mortality/processed/mortality.seqs.train\")\n",
    "train_seqs = pickle.load(open(\"data/mortality/processed/mortality.seqs.train\", 'rb'))\n",
    "codemap = pickle.load(open(\"data/mortality/processed/mortality.codemap.train\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted(codemap.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_seqs:\n",
    "    matrix = np.zeros((len(i), 911))\n",
    "    for pos, j in enumerate(i):\n",
    "        for k in j:\n",
    "            matrix[pos, int(k)] = 1\n",
    "    data.append(csr_matrix(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(df_diagnosis,df_mortality, on='SUBJECT_ID', how='outer')#.dropna(subset=['ICD9_CODE']).loc[:, ['SUBJECT_ID', 'MORTALITY']].drop_duplicates()\n",
    "test.ICD9_CODE = test.ICD9_CODE.apply(convert_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.ICD9_CODE=='656']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_mortality.MORTALITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.resize((3,911))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1,3,4],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.resize(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [(np.array([1,2,3]), 1), (np.array([[2,3,4],[2,3,4]]), 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in t:\n",
    "    x[0].resize(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sort(key = lambda x: x[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hw5] *",
   "language": "python",
   "name": "conda-env-hw5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
